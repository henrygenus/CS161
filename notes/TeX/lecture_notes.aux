\relax 
\bbl@cs{beforestart}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces a course-grain view of an AI agent\relax }}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example deep learning process\relax }}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}LISP}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Solving as Search}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Blind Search Strategies}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Heuristic Search}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Constraint Satisfaction}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Two Player Games}{20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax }}{20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces We can skip any value lower than the min!  This is called \textbf  {\underline  {alpha-beta pruning}}   \indent (alpha for max, beta for minimum).  \noindent Our behavior is dependent on the type of node:  \indent min node — examine children lowest-first  \indent max node — examine children largest-first  \noindent The gain depends on value order.  $\DOTSB \tmspace  +\thickmuskip {.2777em}\DOTSB \Relbar \joinrel \Rightarrow \tmspace  +\thickmuskip {.2777em}$ with a-b pruning, $T = O(b^{m/2})$.  This is up to twice as deep!  This is the best case, the average is $O(b^{3m/4})$. \relax }}{21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   Our choice of cutoff is very important:  \indent A depth cutoff in a search must be \textbf  {\underline  {quiescent}}  $\equiv $ unlikely to drastically change values.  \indent A bad cutoff may fall victim to the \textbf  {\underline  {horizon effect}}  $\equiv $ when negatives are pushed just past the cutoff.  \indent We address this by logging clearly great moves.  Time complexity: $O(b^m n^m)$ for n distinct events.  \newline  \newline  \newline  \relax }}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Knowledge Representation}{22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Classical Propositional Logic}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Modern Propositional Logic}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}First Order Logic Representation}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}First Order Logic Inference}{32}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Probabilistic Reasoning}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Bayesian Networks}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15}Modeling and Inference}{39}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16}Bayesian Learning}{41}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17}Decision Trees and Random Forests}{43}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {18}Neural Networks}{45}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {19}Neural Network Applications}{47}\protected@file@percent }
