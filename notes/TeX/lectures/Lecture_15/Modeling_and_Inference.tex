\documentclass[../../lecture_notes.tex]{subfiles}

\begin{document}

\begin{tikzpicture}
	\node [circle, draw] (C) {C};
	\node [circle, draw, below left =of C] (T1) {$T_1$};
	\node [circle, draw, below right =of C] (T2) {$T_2$};
	\node [circle, draw, below right =of T1] (A) {A};
	\node [circle, draw, above right =of C] (S) {S};
	\draw [->] (S.south west) -- (C.north east);
	\draw [->] (S.south) -- (T2.north);
	\draw [->] (C.south west) -- (T1.north east);
	\draw [->] (C.south east) -- (T2.north west);
	\draw [->] (T2.south west) -- (A.north east);
	\draw [->] (T1.south east) -- (A.north west);
\end{tikzpicture} \begin{tabular} { | c | c | } \hline 
	S & $\theta_S$ \\\hline male & 0.55 \\ female & 0.45 \\\hline\end{tabular}
\begin{center}\begin{tabular} { | c c | c | } \hline
	S & C & $\theta_{c|s}$ \\\hline 
	male & yes & 0.05 \\
	male & no & 0.95 \\
	female & yes & 0.01 \\
	female & no & 0.99
\\\hline\end{tabular}
\begin{tabular} { | c c | c | } \hline
	C & $T_1$ & $\theta_{t_1|c}$ \\\hline 
	yes & +ve & 0.8 \\
	yes & -ve & 0.2 \\
	no & +ve & 0.2 \\
	no & -ve & 0.8
\\\hline\end{tabular}
\begin{tabular} { | c c c | c | } \hline
	S & C & $T_2$ & $\theta_{t_2|c,s}$ \\\hline 
	male & yes & +ve & 0.80 \\
	male & yes & -ve & 0.20 \\
	male & no & +ve & 0.20 \\
	male & no & -ve & 0.80 \\
	female & yes & +ve & 0.95 \\
	female & yes & -ve & 0.05 \\
	female & no & +ve & 0.05 \\
	female & no & -ve & 0.95
\\\hline\end{tabular}
\begin{tabular} { | c c c | c | } \hline
	$T_1$ & $T_2$ & $A$ & $\theta_{a|t_1,t_2}$ \\\hline 
	+ve & +ve & yes & 1 \\
	+ve & +ve & no & 0 \\
	+ve & -ve & yes & 0 \\
	+ve & -ve & no & 1 \\
	-ve & +ve & yes & 0 \\
	-ve & +ve & no & 1 \\
	-ve & -ve & yes & 1 \\
	-ve & -ve & no & 0
\\\hline\end{tabular}\end{center}
\noindent Variables:\\
	\indent C — condition\\
	\indent T’s — tests to detect the condition\\
	\indent S — sex of the patient\\
\\
We first compute the marginal distributions to discuss probabilities.\\
These come in two major forms:

\begin{center}\begin{tikzpicture}
	\node [align=center] (L) {
		\begin{tabular} { | c | c | } \hline $T_1$ & 0.2192 \\ $\neg T_1$ & .7808 \\\hline\end{tabular}};
	\node [align=center, below =0cm of L] {Prior Marginal\\(pre-testing)};
	\node [align=center, right =of L] (R) {
		\begin{tabular} { | c | c | } \hline C & 0.4533 \\ $\neg C$ & .5467 \\\hline\end{tabular}};
	\node [align=center, below =0cm of R] {Posterior Marginal\\(assuming $T_1 = T_2 = 1$)};
\end{tikzpicture}\end{center}

\noindent We then use this information to find the \textbf{\underline{Most Probable Explanation}}:\\
We assume the final consequence (A) and search for the most probable query\\
	\indent Setting A moves us from 32 → 8 states;\\
	\indent result: \{C=no, S=fe, T1=-ve, T2=-ve\} = 47\%\\
\\
This is not applicable in every situation; we generalize into the \textbf{\underline{Maximum a Posteriori Hypothesis}}:\\
	\indent this is more complex and less efficient, but more often applicable\\
	\indent we find a subset of variables \& find the MPE\\
	\indent (ex. S=C | A $\rightarrow$ \{C=no, S=male\} $|approx$ 49.3\%\\
\\
When we seek to make inferences, algorithms fall into two major categories:
\begin{enumerate} [itemsep=0mm]
	\item variable elimination
	\item conditioning
\end{enumerate} \medskip

\noindent In both situations, complexity is tied to the topology of the Bayesian Network\\
	\indent the actual property is \textbf{\underline{tree width}} — it is roughly analogous to connectivity\\
	\indent MPH = $O(nd^w)$ given n=\#var, d=\#val, w=width\\
We will not define tree width, as it is complex, but we observe a few special cases:\\
	\indent Trees have width 1 (~one path from any node to node)\\
	\indent Poly-trees (>1 parent ok) have width = the maximum parent count of any node\\
These are both singly connected networks; multiply connected leads to a DAG\\

\begin{center}\begin{minipage}{0.7\linewidth}
\noindent Our first method of performing inference is
\subsection*{Weighted Model Counting}
\noindent Consider the statement $\Delta = (A \lor B) \land\neg C$\\ 
	\indent This has 3 variables and suggests 8 worlds\\
	\indent Given an nd-DNNF circuit, we can solve in O(N)!\\
	\indent Consider the example on the right; it is trivial! \\
	\indent WMC = 0.04 + 0.10 + 0.00 = 0.14\\
	\indent We thus only need a compiler which would transform $\Delta \rightarrow$ sd-DNNF\\
	\indent Unfortunately, in the general case this is intractable\\
\end{minipage}%
\begin{minipage}{0.3\linewidth}\begin{tabular}{ | c | c | c | c |}\hline
	A & B & C & w\\\hline
	t & t & t & 0.08\\
	t & t & f & 0.04\\
	t & f & t & 0.10\\
	t & f & f & 0.10\\
	f & t & t & 0.20\\
	f & t & f & 0.00\\
	f & f & t & 0.42\\
	f & f & f & 0.06\\\hline
\end{tabular}\end{minipage}\end{center}
	
There are tractable subsets of this, though:

\begin{center}\begin{minipage}{0.2\linewidth}\begin{tikzpicture}
	\node[circle, draw] (A) {A};
	\node[circle, draw, above right =of A] (C) {C};
	\node[circle, draw, below right =of A] (B) {B};
	\draw [->] (A.north east) -- (C.south west);
	\draw [->] (A.south east) -- (B.north west);
\end{tikzpicture}\end{minipage}%
\begin{minipage}{0.8\linewidth}
\noindent We can use probability is our weights! $ \rightarrow WMC(\Delta\land\alpha) = Pr(\alpha)$\\
So we need to convert $\Delta \rightarrow$boolean circuit.\\
We can see that $\Delta \text{ holds in } w_1, w_4, \& w_7$\\
We thus say $W(A) = W(\neg A) = ... = W(\neg C) = 1$\\
We then let $W(P_i) = \theta_i \& W(\neg P_i) = 0, \text{ so } W(A) = W(P_1) W(P_4) W(P_7)$\\
\end{minipage}\end{center}

Thus we can solve probabilistic reasoning by symbol manipulations!\\
Properties of the algorithm:
	\begin{enumerate} [itemsep=0mm]
		\item there are many possible representations
		\item it is not sensitive to tree width
		\item it is not only applicable to Bayesian Networks
	\end{enumerate} \medskip

\noindent We can see that the size(Bayes) = size(CPT);
	though size(Bayes) = O($nd^{k+1}$), size(joint-table) = O($D^n$)!
This shows that Bayesian Networks are much more space efficient!\\
\\
The process of modeling logic as a Bayesian Network has 3 steps:
\begin{enumerate} [itemsep=0mm]
	\item define variables \& values
	\item define edges
	\item specify CPT
\end{enumerate} 

\noindent Variables will then be labeled either query or evidence variables depending on the query.\\
This is a common approach used in early spam filters and Google ad Rephil.

\begin{center}\begin{minipage}{0.5\linewidth}
\noindent Consider the following statements
\begin{itemize} [itemsep=0mm]
	\item The cold causes a sore throat/chill
	\item The flu causes a sore throat/chill/fever/body ache
	\item Tonsillitis can show itself in fever/body ache
\end{itemize}
\noindent This network forms a bipartite graph.\\
We could have used one multivariable disease node.\\
We use this to give probabilities of a given condition.
\end{minipage}%
\begin{minipage}{0.5\linewidth}\begin{tikzpicture}
	\node [circle, draw, minimum height=1.5cm] (Co) {Cold};
	\node [circle, draw, right =of Co, minimum height=1.5cm] (Fl) {Flu};
	\node [circle, draw, right =of Fl, minimum height=1.5cm] (T) {Tonsilitis};
	\node [circle, draw, below left = of Fl, minimum height=1.5cm] (Ch) {Chill};
	\node [circle, draw, below right = of Fl, minimum height=1.5cm] (Fe) {Fever};
	\node [circle, draw, left =of Ch, align=center, minimum height=1.5cm] (ST) {Sore\\Throat};
	\node [circle, draw, right =of Fe, align=center, minimum height=1.5cm] (BA) {Body\\Ache};
	\draw [->] (Co.south) -- (ST.north east);
	\draw [->] (Co.south) -- (Ch.north);
	\draw [->] (Fl.south) -- (Ch.north);
	\draw [->] (Fl.south) -- (Fe.north);
	\draw [->] (T.south) -- (Fe.north);
	\draw [->] (T.south) -- (BA.north west);
	\draw [->] (Fl.south) -- (ST.north east);
	\draw [->] (Fl.south) -- (BA.north west);
\end{tikzpicture}\end{minipage}\end{center}

If we have complete information, we can model a system as a Bayesian Network.\\
For incomplete information, however, we must use Expectation Maximization.\\
For example, suppose the following environment:

\begin{center}\begin{minipage}{0.5\textwidth}\begin{tikzpicture}
	\node[circle, draw, align=center] (P) {Prog\\ (P)};
	\node[circle, draw, below right =of P, align=center] (S) {Scan\\ (S)};
	\node[circle, draw, below left =of P, align=center] (L) {Prog\\ (L)};
	\node[circle, draw, below left =of L, align=center] (U) {Urine\\ (U)};
	\node[circle, draw, below right =of L, align=center] (B) {Blood\\ (B)};
	\draw [->] (P.south east) -- (S.north west);
	\draw [->] (P.south west) -- (L.north east);
	\draw [->] (L.south east) -- (B.north west);
	\draw [->] (L.south west) -- (U.north east);
\end{tikzpicture}\end{minipage}%
\begin{minipage}{0.5\textwidth}\begin{tikzpicture}
	\node[rectangle, draw, align=center] (1) {P(P|S) = 0.87\\
					P($S|\neg P$) = 0.01\ \&\ P($\neg S|P$) = 0.1\\
					P($U|\neg P$) = 0.1\ \&\ P($\neg U|P$) = 0.3\\
					P($B|\neg P$) = 0.1\ \&\ P($\neg B|P$) = 0.2};
	\node[below =0.2 of 1, align=center] (2) {
		\begin{tabular} { | c | c | }\hline P(p) & 0.87\\\hline P($\neg$p) & 0.13\\\hline\end{tabular}};
	\node[below =0.2 of 2, align=center] (3) { \begin{tabular} { | c | c | c | }
		\hline S & P & P(S, P)\\\hline 1 & 1 & 0.9\\1 & 0 & 0.1\\0 & 1 & 0.01\\0 & 0 & 0.99\\\hline\end{tabular}};
\end{tikzpicture}\end{minipage}\end{center}
\noindent What is the marginal $(P|\neg S, \neg B, \neg U)$? -- 10.21\%!\\
WHAT?? why is that so high?\\
It turns out this is because of the false negative rate of the scanning test.\\
We want a false negative rate of below 5\%.\\
We can address this in one of three ways:
	\begin{enumerate} [itemsep=0mm]
		\item get a better scanning test — a false (-) for S of 4.63\% meets our requirement
		\item lower the success of the procedure — 75.59\% would meet our requirement
		\item increase P(L|P) — 99.67\% meets our requirement
	\end{enumerate}
	(b) and (c) turn out to be either impractical or in-economical, so our standard approach is to pay!

\end{document}